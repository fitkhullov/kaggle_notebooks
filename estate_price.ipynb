{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-04T08:44:37.312826Z","iopub.execute_input":"2021-08-04T08:44:37.31321Z","iopub.status.idle":"2021-08-04T08:44:37.321607Z","shell.execute_reply.started":"2021-08-04T08:44:37.313173Z","shell.execute_reply":"2021-08-04T08:44:37.320606Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor, VotingRegressor, BaggingRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import r2_score as r2\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\n\nfrom datetime import datetime\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:45.820687Z","iopub.execute_input":"2021-08-04T08:44:45.821288Z","iopub.status.idle":"2021-08-04T08:44:45.879225Z","shell.execute_reply.started":"2021-08-04T08:44:45.82125Z","shell.execute_reply":"2021-08-04T08:44:45.878355Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matplotlib.rcParams.update({'font.size': 14})","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:50.292844Z","iopub.execute_input":"2021-08-04T08:44:50.293193Z","iopub.status.idle":"2021-08-04T08:44:50.298147Z","shell.execute_reply.started":"2021-08-04T08:44:50.293163Z","shell.execute_reply":"2021-08-04T08:44:50.297152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):\n    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))\n    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))\n    \n    plt.figure(figsize=(18,10))\n    \n    plt.subplot(121)\n    sns.scatterplot(x=train_pred_values, y=train_true_values)\n    plt.xlabel('Predicted values')\n    plt.ylabel('True values')\n    plt.title('Train sample prediction')\n    \n    plt.subplot(122)\n    sns.scatterplot(x=test_pred_values, y=test_true_values)\n    plt.xlabel('Predicted values')\n    plt.ylabel('True values')\n    plt.title('Test sample prediction')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:51.000662Z","iopub.execute_input":"2021-08-04T08:44:51.001179Z","iopub.status.idle":"2021-08-04T08:44:51.008498Z","shell.execute_reply.started":"2021-08-04T08:44:51.001145Z","shell.execute_reply":"2021-08-04T08:44:51.007557Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '../input/real-estate-price-prediction-moscow/train.csv'\nTEST_DATASET_PATH = '../input/real-estate-price-prediction-moscow/test.csv'","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:52.224979Z","iopub.execute_input":"2021-08-04T08:44:52.225318Z","iopub.status.idle":"2021-08-04T08:44:52.229289Z","shell.execute_reply.started":"2021-08-04T08:44:52.225287Z","shell.execute_reply":"2021-08-04T08:44:52.228315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.<b> Считывание данных","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATASET_PATH)\ntest_df = pd.read_csv(TEST_DATASET_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:53.452808Z","iopub.execute_input":"2021-08-04T08:44:53.45328Z","iopub.status.idle":"2021-08-04T08:44:53.546684Z","shell.execute_reply.started":"2021-08-04T08:44:53.453248Z","shell.execute_reply":"2021-08-04T08:44:53.545812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>Square","metadata":{"execution":{"iopub.status.busy":"2021-07-25T09:51:34.657587Z","iopub.execute_input":"2021-07-25T09:51:34.658159Z","iopub.status.idle":"2021-07-25T09:51:34.66892Z","shell.execute_reply.started":"2021-07-25T09:51:34.65811Z","shell.execute_reply":"2021-07-25T09:51:34.66785Z"}}},{"cell_type":"markdown","source":"### Т.к. у нас в площади квартиры были значения, меньшие 18 кв.м., и большие 100 кв.м., то  их было решено заменить на минимально-адекватные (для меньших) и медианные (для больших). Это позволило снизить разность между мат.ожиданием и медианной на ~ 50%.","metadata":{}},{"cell_type":"markdown","source":"### <b> KitchenSquare\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T12:09:56.417935Z","iopub.execute_input":"2021-07-25T12:09:56.418373Z","iopub.status.idle":"2021-07-25T12:09:56.495318Z","shell.execute_reply.started":"2021-07-25T12:09:56.418332Z","shell.execute_reply":"2021-07-25T12:09:56.494212Z"}}},{"cell_type":"markdown","source":"### Приступаем сначала к анализу признака KitchenSquare, потому что с помощью него будем восстанавливать пропущенные значения в признаке LifeSquare. Значения, превышающие 0.975 квантиль заменяем на медиану, а значения, меньшие 3 кв.м. заменяем на 3 кв.м.","metadata":{}},{"cell_type":"markdown","source":"### <b>HouseFloor, Floor\n","metadata":{}},{"cell_type":"markdown","source":"### Выбросами у признака HouseFloor будем считать значения, равные нулю и будем заполнять их медианой.\n### Выбросами у признака Floor будем считать такие значения, этаж квартиры которых больше этажности дома. Эти значения будем заменять на случайное число в диапозоне от 1 до x, где x - кол-во этажей в доме.\n### В результате такой обработки мат.ожидание Floor увеличилось на 4%.\n### Медиана и мат.ожидание признака HouseFloor свдинулись вправо.\n","metadata":{}},{"cell_type":"markdown","source":"### <b>HouseYear","metadata":{}},{"cell_type":"markdown","source":"### Значения, большие 2020 г. постройки — заменим на 2020.\n### Значения, меньшие 1800 г. постройки — заменим на медиану.","metadata":{}},{"cell_type":"markdown","source":"### 3.<b>  Обработка пропусков","metadata":{}},{"cell_type":"markdown","source":"### <b>LifeSquare","metadata":{"execution":{"iopub.status.busy":"2021-07-26T08:28:53.306206Z","iopub.execute_input":"2021-07-26T08:28:53.306639Z","iopub.status.idle":"2021-07-26T08:28:53.326136Z","shell.execute_reply.started":"2021-07-26T08:28:53.306602Z","shell.execute_reply":"2021-07-26T08:28:53.324848Z"}}},{"cell_type":"markdown","source":"### Пропуски этого признака можно вычислить как разность между Square и KitchenSquare, если для соответствующего NaN есть значения Square и KitchenSquare. Т.к. пропусков в полях Square и KitchenSquare нет, то мы сможем восстановить все поля, но сналача оценим СКО такой замены по имеющимся данным.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error as mse","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:58.400937Z","iopub.execute_input":"2021-08-04T08:44:58.40131Z","iopub.status.idle":"2021-08-04T08:44:58.40587Z","shell.execute_reply.started":"2021-08-04T08:44:58.401274Z","shell.execute_reply":"2021-08-04T08:44:58.404714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cond = ~train_df['LifeSquare'].isna()\npred = train_df.loc[cond, 'Square'] - train_df.loc[cond, 'KitchenSquare']\nrmse = mse(train_df.loc[cond, 'LifeSquare'], pred)**0.5\nprint(f'RMSE : {rmse}')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:58.697023Z","iopub.execute_input":"2021-08-04T08:44:58.697504Z","iopub.status.idle":"2021-08-04T08:44:58.728059Z","shell.execute_reply.started":"2021-08-04T08:44:58.697473Z","shell.execute_reply":"2021-08-04T08:44:58.727067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"M(X) : {train_df['LifeSquare'].mean()}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:59.064884Z","iopub.execute_input":"2021-08-04T08:44:59.065218Z","iopub.status.idle":"2021-08-04T08:44:59.069589Z","shell.execute_reply.started":"2021-08-04T08:44:59.065187Z","shell.execute_reply":"2021-08-04T08:44:59.068923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Среднее значене LifeSquare ~ 37, а СКО, при такой оценке LifeSquare, больше среднего ~ в 2,5 раза. Попробуем придумать другую замену.\n### Посчитаем средний процент разности между Square и LifeSquare + KitchenSquare.","metadata":{"execution":{"iopub.status.busy":"2021-07-26T09:06:03.983066Z","iopub.execute_input":"2021-07-26T09:06:03.983697Z","iopub.status.idle":"2021-07-26T09:06:03.990363Z","shell.execute_reply.started":"2021-07-26T09:06:03.983643Z","shell.execute_reply":"2021-07-26T09:06:03.989293Z"}}},{"cell_type":"code","source":"cond = ~train_df['LifeSquare'].isna()\nsub_1 = (train_df.loc[cond, 'Square'] - (train_df.loc[cond, 'LifeSquare'] + train_df.loc[cond, 'KitchenSquare']))/train_df.loc[cond, 'Square']\nsub_1.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:03.032857Z","iopub.execute_input":"2021-08-04T08:45:03.033387Z","iopub.status.idle":"2021-08-04T08:45:03.056129Z","shell.execute_reply.started":"2021-08-04T08:45:03.033334Z","shell.execute_reply":"2021-08-04T08:45:03.055481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### т.е. в среднем мы ошибаемся ~ на 18%. Почистим данные от выбросов, чтобы посмотреть как изменится средняя ошибка в %.\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T05:26:48.439303Z","iopub.execute_input":"2021-07-27T05:26:48.439545Z","iopub.status.idle":"2021-07-27T05:26:48.458505Z","shell.execute_reply.started":"2021-07-27T05:26:48.439521Z","shell.execute_reply":"2021-07-27T05:26:48.457954Z"}}},{"cell_type":"code","source":"train_df['LifeSquare_outlier'] = 0\n\ncond_1 = train_df['LifeSquare'] < 10\ncond_2 = train_df['LifeSquare'] > train_df.loc[cond, 'LifeSquare'].quantile(q = .975)\n\ntrain_df.loc[cond & (cond_1 | cond_2), 'LifeSquare_outlier'] = 1\ntrain_df.loc[cond & cond_1, 'LifeSquare'] = 10\ntrain_df.loc[cond & cond_2, 'LifeSquare'] = train_df['LifeSquare'].median()\n\nsub_2 = (train_df.loc[cond, 'Square'] - (train_df.loc[cond, 'LifeSquare'] + train_df.loc[cond, 'KitchenSquare']))/train_df.loc[cond, 'Square']\nsub_2.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:07.580792Z","iopub.execute_input":"2021-08-04T08:45:07.581361Z","iopub.status.idle":"2021-08-04T08:45:07.601843Z","shell.execute_reply.started":"2021-08-04T08:45:07.581311Z","shell.execute_reply":"2021-08-04T08:45:07.600904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Такие замены привели к увеличению мат.ожидания ~ на 16%, но к значительному уменьшению СКО, что можно воспринимать как то, что такая замена для LifeSquare стала более точной.\n### Т.к. разность между Square и (LifeSquare + KitchenSquare) имеет тенденцию сохраняться ~ на уровне 20% от Square, то продолжим её сохранять.","metadata":{}},{"cell_type":"code","source":"train_df.loc[~cond, 'LifeSquare'] = 0.8*train_df.loc[~cond, 'Square'] - train_df.loc[~cond, 'KitchenSquare']\n\ntrain_df[['LifeSquare', 'Square', 'KitchenSquare']].plot(kind = 'box', subplots = True, figsize = (15, 7))\nplt.subplots_adjust(wspace = 0.5)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:11.117033Z","iopub.execute_input":"2021-08-04T08:45:11.117406Z","iopub.status.idle":"2021-08-04T08:45:11.548613Z","shell.execute_reply.started":"2021-08-04T08:45:11.11736Z","shell.execute_reply":"2021-08-04T08:45:11.547701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>Попытка восстановить пропущенные значения признака LifeSquare при помощи моделей ML","metadata":{}},{"cell_type":"markdown","source":"<b> Для восстановления пропущенных значений было использовано 4 метода: LinearRegression, RandomForest, SVR, GBR. <br>Сначала данные из тестового датасета были очищены от выбросов по формулам<br> LifeSquare = 0.8*Sqaure - KitchenSquare (для значений LifeSquare меньших 10)<br>\nи<br>LifeSquare = LifeSquare.median (для значений, превышающих 975 квантиль)<br>\nДалее данные были разбиты на 3 датасета X_train и X_valid LS_nan.\nНиже представлены лучшие результаты среднего значения R2 на кросс-валидации с 3 фолдами для выбранных методов:<br>\nLR: 0.6875<br>\nRF: 0.7746<br>\nSVR: 0.1523<br>\nGBR: 0.7808<br>\nКандидатами для восстановления пропущенных значений стали GBR и RF.\nДалее я объединил X_train и X_valid для прогнозирования пропусков.<br>\nНиже представлены лучшие результаты среднего значения R2 на кросс-валидации с 3 фолдами для отобранных методов.<br>\nGBR: 0.7826<br>\nRF: 0.7898<br>\nДалее была предсказана целевая переменная Price. Наиболее высокую метрику показала следующая комбинация:<br>\nВосстановление пропусков LifeSquare по GBR, предсказание целевой переменной при помощи GBR. Среднее значение R2 на CV = 0.7478<br>\nТем не менее, эта работа не привела к значительному увеличению среднего значения на этапе валидации — выигрыш по сравнению с дефолтной обработкой LifeSquare составил ~ 5%, но это, все таки, лучше. \n","metadata":{}},{"cell_type":"code","source":"df_results_NAN_RF = pd.DataFrame({'Metrics' : ['R2_mean', 'D(X)', 'RMSE(X)'], 'RF' : [0.7458, 0.00045, 0.0212], 'GBR' : [0.7475, 0.00017, 0.0129]}, columns = ['Metrics', 'RF', 'GBR'])\n\ndf_results_NAN_GBR = pd.DataFrame({'Metrics' : ['R2_mean', 'D(X)', 'RMSE(X)'], 'RF' : [0.7454, 0.00043, 0.0208], 'GBR' : [0.7478, 0.00018, 0.0134]}, columns = ['Metrics', 'RF', 'GBR'])","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:15.649053Z","iopub.execute_input":"2021-08-04T08:45:15.649442Z","iopub.status.idle":"2021-08-04T08:45:15.659487Z","shell.execute_reply.started":"2021-08-04T08:45:15.649406Z","shell.execute_reply":"2021-08-04T08:45:15.658407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>Data Preprocessing for LifeSquare","metadata":{}},{"cell_type":"code","source":"class DataPreprocessing_for_LS:\n    \"\"\"Подготовка исходных данных\"\"\"\n    \n    def __init__(self):\n        \"\"\"Параметры класса\"\"\"\n        \n        self.medians = None\n        self.kitchen_square_quantile = None        \n        \n    def fit(self, X):\n        \"\"\"Сохранение статистик\"\"\"\n        \n        self.medians = X.median()\n        self.kitchen_square_quantile = X['KitchenSquare'].quantile(.975)        \n                \n    def transform(self, X):\n        \"\"\"Трансформация данных\"\"\"\n        \n        X = X.copy()\n        \n        \n        # Rooms\n        X['Rooms_outlier'] = 0\n        X.loc[(X['Rooms'] == 0) | (X['Rooms'] >= 6), 'Rooms_outlier'] = 1\n        X.loc[X['Rooms'] == 0, 'Rooms'] = 1\n        X.loc[X['Rooms'] >= 6, 'Rooms'] = self.medians['Rooms']\n        \n        \n        # Square\n        X['Square_outlier'] = 0\n        X.loc[(X['Square'] < 18.0) | (X['Square'] > 100.0), 'Square_outlier'] = 1\n        X.loc[X['Square'] < 18.0, 'Square'] = 18.0\n        X.loc[X['Square'] > 100.0, 'Square'] = self.medians['Square']\n\n        \n        # KitchenSquare\n        X['KitchenSquare_outlier'] = 0\n        X.loc[(X['KitchenSquare'] > self.kitchen_square_quantile) | (X['KitchenSquare'] < 3) , 'KitchenSquare_outlier'] = 1\n\n        X.loc[X['KitchenSquare'] > self.kitchen_square_quantile, 'KitchenSquare'] = self.medians['KitchenSquare']\n        X.loc[X['KitchenSquare'] < 3, 'KitchenSquare'] = 3\n        \n        \n        # HouseFloor, Floor\n        X['HouseFloor_outlier'] = 0\n        X['Floor_outlier'] = 0\n\n        X.loc[X['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\n        X.loc[X['Floor'] > X['HouseFloor'], 'Floor_outlier'] = 1\n\n        X.loc[X['HouseFloor'] == 0, 'HouseFloor'] = self.medians['HouseFloor']\n\n        floor_outliers = X.loc[X['Floor'] > X['HouseFloor']].index\n        X.loc[floor_outliers, 'Floor'] = X.loc[floor_outliers, 'HouseFloor'].apply(lambda x: random.randint(1, x))\n        \n        \n        # HouseYear\n        current_year = datetime.now().year\n        X['HouseYear_outlier'] = 0\n        X.loc[(X['HouseYear'] == 4.968000e+03) | (X['HouseYear'] > current_year) | (X['HouseYear'] < 1800), 'HouseYear_outlier'] = 1\n\n        X.loc[X['HouseYear'] == 4.968000e+03, 'HouseYear'] = 1968\n        X.loc[X['HouseYear'] > current_year, 'HouseYear'] = current_year\n        X.loc[X['HouseYear'] < 1800, 'HouseYear'] = self.medians['HouseYear']        \n        \n        \n        # Healthcare_1\n        if 'Healthcare_1' in X.columns:\n            X.drop('Healthcare_1', axis=1, inplace=True)  \n\n        \n        # Default\n        X.fillna(self.medians, inplace = True)\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:19.337727Z","iopub.execute_input":"2021-08-04T08:45:19.338069Z","iopub.status.idle":"2021-08-04T08:45:19.466099Z","shell.execute_reply.started":"2021-08-04T08:45:19.338038Z","shell.execute_reply":"2021-08-04T08:45:19.465032Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b> Feature Generator for LS","metadata":{}},{"cell_type":"code","source":"class FeatureGenetator_for_LS():\n    \"\"\"Генерация новых фич\"\"\"\n    \n    def __init__(self):\n        self.DistrictId_counts = None\n        self.binary_to_numbers = None\n        self.med_life_square_by_district = None\n        self.med_life_square_by_floor_year = None\n        self.house_year_max = None\n        self.floor_max = None\n        self.district_size = None\n        self.district_size_median = None\n        \n    def fit(self, X, y=None):        \n        X = X.copy()\n        \n        \n        # Binary features\n        self.binary_to_numbers = {'A': 0, 'B': 1}\n        \n        \n        # DistrictSize\n        self.district_size = X['DistrictId'].value_counts().reset_index().rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n        self.district_size_median = self.district_size['DistrictSize'].median()\n                \n            \n        # Target encoding        \n        df = X.copy()\n        \n        if y is not None:\n        ## District, Rooms\n            df['LifeSquare'] = y.values\n            \n            self.med_life_square_by_district = df.groupby(['DistrictId', 'Rooms'], as_index=False).agg({'LifeSquare':'median'})\\\n                                            .rename(columns={'LifeSquare':'MedLifeSquareByDistrict'})\n            \n            self.med_life_square_by_district_median = self.med_life_square_by_district['MedLifeSquareByDistrict'].median()\n            \n        ## floor, year\n            self.floor_max = df['Floor'].max()\n            self.house_year_max = df['HouseYear'].max()\n            \n            df = self.floor_to_cat(df)\n            df = self.year_to_cat(df)\n            \n            self.med_life_square_by_floor_year = df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'LifeSquare':'median'}).\\\n                                            rename(columns={'LifeSquare':'MedLifeSquareByFloorYear'})\n            self.med_life_square_by_floor_year_median = self.med_life_square_by_floor_year['MedLifeSquareByFloorYear'].median()\n        \n\n        \n    def transform(self, X):\n        \n        # Binary features\n        X['Ecology_2'] = X['Ecology_2'].map(self.binary_to_numbers)  # self.binary_to_numbers = {'A': 0, 'B': 1}\n        X['Ecology_3'] = X['Ecology_3'].map(self.binary_to_numbers)\n        X['Shops_2'] = X['Shops_2'].map(self.binary_to_numbers)\n    \n        # DistrictId, IsDistrictLarge\n        X = X.merge(self.district_size, on='DistrictId', how='left')\n    \n        \n        X['IsDistrictLarge'] = (X['DistrictSize'] > 100).astype(int)\n        \n        # More categorical features\n        X = self.floor_to_cat(X)  # + столбец floor_cat\n        X = self.year_to_cat(X)   # + столбец year_cat\n        \n        # Target encoding\n        if self.med_life_square_by_district is not None:\n            X = X.merge(self.med_life_square_by_district, on=['DistrictId', 'Rooms'], how='left')\n            X['MedLifeSquareByDistrict'].fillna(self.med_life_square_by_district_median, inplace=True)\n            \n        if self.med_life_square_by_floor_year is not None:\n            X = X.merge(self.med_life_square_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\n            X['MedLifeSquareByFloorYear'].fillna(self.med_life_square_by_floor_year_median, inplace=True)\n            \n        # Default\n        X['DistrictSize'].fillna(self.district_size_median, inplace = True)\n        \n        \n        return X\n    \n    def floor_to_cat(self, X):\n        bins = [0, 3, 5, 9, 15, self.floor_max]\n        X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n\n        X['floor_cat'].fillna(-1, inplace=True)\n        return X\n     \n    def year_to_cat(self, X):\n        bins = [0, 1941, 1945, 1980, 2000, 2010, self.house_year_max]\n        X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n\n        X['year_cat'].fillna(-1, inplace=True)\n        return X\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:24.197748Z","iopub.execute_input":"2021-08-04T08:45:24.19826Z","iopub.status.idle":"2021-08-04T08:45:24.218082Z","shell.execute_reply.started":"2021-08-04T08:45:24.198205Z","shell.execute_reply":"2021-08-04T08:45:24.21728Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b> Data Preprocessing","metadata":{}},{"cell_type":"code","source":"class DataPreprocessing:\n    \"\"\"Подготовка исходных данных\"\"\"\n    \n    def __init__(self):\n        \"\"\"Параметры класса\"\"\"\n        \n        self.medians = None\n        self.kitchen_square_quantile = None\n        self.life_square_quantile = None\n        \n    def fit(self, X):\n        \"\"\"Сохранение статистик\"\"\"\n        \n        self.medians = X.median()\n        self.kitchen_square_quantile = X['KitchenSquare'].quantile(.975)\n        self.life_square_quantile = X.loc[~X['LifeSquare'].isna(), 'LifeSquare'].quantile(.975)\n        \n    def transform(self, X):\n        \"\"\"Трансформация данных\"\"\"\n        \n        X = X.copy()\n        \n        \n        # Rooms\n        X['Rooms_outlier'] = 0\n        X.loc[(X['Rooms'] == 0) | (X['Rooms'] >= 6), 'Rooms_outlier'] = 1\n        X.loc[X['Rooms'] == 0, 'Rooms'] = 1\n        X.loc[X['Rooms'] >= 6, 'Rooms'] = self.medians['Rooms']\n        \n        \n        # Square\n        X['Square_outlier'] = 0\n        X.loc[(X['Square'] < 18.0) | (X['Square'] > 100.0), 'Square_outlier'] = 1\n        X.loc[X['Square'] < 18.0, 'Square'] = 18.0\n        X.loc[X['Square'] > 100.0, 'Square'] = self.medians['Square']\n\n        \n        # KitchenSquare\n        X['KitchenSquare_outlier'] = 0\n        X.loc[(X['KitchenSquare'] > self.kitchen_square_quantile) | (X['KitchenSquare'] < 3) , 'KitchenSquare_outlier'] = 1\n\n        X.loc[X['KitchenSquare'] > self.kitchen_square_quantile, 'KitchenSquare'] = self.medians['KitchenSquare']\n        X.loc[X['KitchenSquare'] < 3, 'KitchenSquare'] = 3\n        \n        \n        # HouseFloor, Floor\n        X['HouseFloor_outlier'] = 0\n        X['Floor_outlier'] = 0\n\n        X.loc[X['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\n        X.loc[X['Floor'] > X['HouseFloor'], 'Floor_outlier'] = 1\n\n        X.loc[X['HouseFloor'] == 0, 'HouseFloor'] = self.medians['HouseFloor']\n\n        floor_outliers = X.loc[X['Floor'] > X['HouseFloor']].index\n        X.loc[floor_outliers, 'Floor'] = X.loc[floor_outliers, 'HouseFloor'].apply(lambda x: random.randint(1, x))\n        \n        \n        # HouseYear\n        current_year = datetime.now().year\n        X['HouseYear_outlier'] = 0\n        X.loc[(X['HouseYear'] == 4.968000e+03) | (X['HouseYear'] > current_year) | (X['HouseYear'] < 1800), 'HouseYear_outlier'] = 1\n\n        X.loc[X['HouseYear'] == 4.968000e+03, 'HouseYear'] = 1968\n        X.loc[X['HouseYear'] > current_year, 'HouseYear'] = current_year\n        X.loc[X['HouseYear'] < 1800, 'HouseYear'] = self.medians['HouseYear']        \n        \n        \n        # Healthcare_1\n        if 'Healthcare_1' in X.columns:\n            X.drop('Healthcare_1', axis=1, inplace=True)\n        \n        \n        # LifeSquare\n        X['LifeSquare_outlier'] = 0        \n\n        X.loc[(~X['LifeSquare'].isna()) & ((X['LifeSquare'] < 10) | (X['LifeSquare'] > self.life_square_quantile)), 'LifeSquare_outlier'] = 1    \n        X.loc[(~X['LifeSquare'].isna()) & (X['LifeSquare'] < 10), 'LifeSquare'] = 0.8*X.loc[~X['LifeSquare'].isna(), 'Square'] - X.loc[~X['LifeSquare'].isna(), 'KitchenSquare']\n        X.loc[(~X['LifeSquare'].isna()) & (X['LifeSquare'] > self.life_square_quantile), 'LifeSquare'] = self.medians['LifeSquare']\n        \n        X.loc[X['LifeSquare'].isna(), 'LifeSquare'] = 0.8*X.loc[X['LifeSquare'].isna(), 'Square'] - X.loc[X['LifeSquare'].isna(), 'KitchenSquare']\n        \n        # Default\n        X.fillna(self.medians, inplace = True)\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:28.761862Z","iopub.execute_input":"2021-08-04T08:45:28.762417Z","iopub.status.idle":"2021-08-04T08:45:28.785319Z","shell.execute_reply.started":"2021-08-04T08:45:28.762365Z","shell.execute_reply":"2021-08-04T08:45:28.784329Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. <b>Построение новых признаков","metadata":{}},{"cell_type":"code","source":"class FeatureGenetator():\n    \"\"\"Генерация новых фич\"\"\"\n    \n    def __init__(self):\n        self.DistrictId_counts = None\n        self.binary_to_numbers = None\n        self.med_price_by_district = None\n        self.med_price_by_floor_year = None\n        self.house_year_max = None\n        self.floor_max = None\n        self.district_size = None\n        self.district_size_median = None\n        \n    def fit(self, X, y=None):        \n        X = X.copy()\n        \n        \n        # Binary features\n        self.binary_to_numbers = {'A': 0, 'B': 1}\n        \n        \n        # DistrictSize\n        self.district_size = X['DistrictId'].value_counts().reset_index().rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n        self.district_size_median = self.district_size['DistrictSize'].median()\n                \n            \n        # Target encoding        \n        df = X.copy()\n        \n        if y is not None:\n        ## District, Rooms\n            df['Price'] = y.values\n            \n            self.med_price_by_district = df.groupby(['DistrictId', 'Rooms'], as_index=False).agg({'Price':'median'})\\\n                                            .rename(columns={'Price':'MedPriceByDistrict'})\n            \n            self.med_price_by_district_median = self.med_price_by_district['MedPriceByDistrict'].median()\n            \n        ## floor, year\n            self.floor_max = df['Floor'].max()\n            self.house_year_max = df['HouseYear'].max()\n            \n            df = self.floor_to_cat(df)\n            df = self.year_to_cat(df)\n            \n            self.med_price_by_floor_year = df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'Price':'median'}).\\\n                                            rename(columns={'Price':'MedPriceByFloorYear'})\n            self.med_price_by_floor_year_median = self.med_price_by_floor_year['MedPriceByFloorYear'].median()\n        \n\n        \n    def transform(self, X):\n        \n        # Binary features\n        X['Ecology_2'] = X['Ecology_2'].map(self.binary_to_numbers)  # self.binary_to_numbers = {'A': 0, 'B': 1}\n        X['Ecology_3'] = X['Ecology_3'].map(self.binary_to_numbers)\n        X['Shops_2'] = X['Shops_2'].map(self.binary_to_numbers)\n        \n        # DistrictId, IsDistrictLarge\n        X = X.merge(self.district_size, on='DistrictId', how='left')\n    \n        \n        X['IsDistrictLarge'] = (X['DistrictSize'] > 100).astype(int)\n        \n        # More categorical features\n        X = self.floor_to_cat(X)  # + столбец floor_cat\n        X = self.year_to_cat(X)   # + столбец year_cat\n        \n        # Target encoding\n        if self.med_price_by_district is not None:\n            X = X.merge(self.med_price_by_district, on=['DistrictId', 'Rooms'], how='left')\n            X['MedPriceByDistrict'].fillna(self.med_price_by_district_median, inplace=True)\n            \n        if self.med_price_by_floor_year is not None:\n            X = X.merge(self.med_price_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\n            X['MedPriceByFloorYear'].fillna(self.med_price_by_floor_year_median, inplace=True)\n            \n        # Default\n        X['DistrictSize'].fillna(self.district_size_median, inplace = True)\n        \n        \n        return X\n    \n    def floor_to_cat(self, X):\n        bins = [0, 3, 5, 9, 15, self.floor_max]\n        X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n\n        X['floor_cat'].fillna(-1, inplace=True)\n        return X\n     \n    def year_to_cat(self, X):\n        bins = [0, 1941, 1945, 1980, 2000, 2010, self.house_year_max]\n        X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n\n        X['year_cat'].fillna(-1, inplace=True)\n        return X\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:31.181822Z","iopub.execute_input":"2021-08-04T08:45:31.182173Z","iopub.status.idle":"2021-08-04T08:45:31.202606Z","shell.execute_reply.started":"2021-08-04T08:45:31.182143Z","shell.execute_reply":"2021-08-04T08:45:31.201545Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.<b> Отбор признаков","metadata":{}},{"cell_type":"markdown","source":"<b> Все признаки","metadata":{}},{"cell_type":"code","source":"feature_names = ['Rooms', 'Square', 'LifeSquare', 'KitchenSquare', 'Floor', 'HouseFloor', 'HouseYear',\n                 'Ecology_1', 'Ecology_2', 'Ecology_3', 'Social_1', 'Social_2', 'Social_3',\n                 'Helthcare_2', 'Shops_1', 'Shops_2']\n\nnew_feature_names = ['Rooms_outlier', 'HouseFloor_outlier', 'HouseYear_outlier', 'DistrictSize',\n                     'IsDistrictLarge',  'MedPriceByDistrict', 'MedPriceByFloorYear']\n\ntarget_name = 'Price'","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:35.693056Z","iopub.execute_input":"2021-08-04T08:45:35.693407Z","iopub.status.idle":"2021-08-04T08:45:35.698943Z","shell.execute_reply.started":"2021-08-04T08:45:35.693375Z","shell.execute_reply":"2021-08-04T08:45:35.69764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b> Обучение моделей RF/GBR для восстановления NaN у LifeSquare на всем датасете (без валидационного набора)","metadata":{}},{"cell_type":"code","source":"feature_names = ['Square', 'Price', 'KitchenSquare', 'Floor', 'HouseFloor', 'HouseYear',\n                 'Ecology_1', 'Social_1', 'Social_2', 'Social_3']\n\nnew_feature_names = ['DistrictSize', 'MedLifeSquareByFloorYear', 'MedLifeSquareByDistrict']\n\ntarget_name = 'LifeSquare'\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:40.241025Z","iopub.execute_input":"2021-08-04T08:45:40.241676Z","iopub.status.idle":"2021-08-04T08:45:40.247672Z","shell.execute_reply.started":"2021-08-04T08:45:40.241607Z","shell.execute_reply":"2021-08-04T08:45:40.246665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(TRAIN_DATASET_PATH) #Считываем данные\ncond = data['LifeSquare'].isna()\n\nLS_nan = data.loc[cond, ] #Разбиваем на 2 df'a: с пропусками и нет\nLS_nan.drop(columns = target_name, inplace = True) #Выбиваем целевую переменную \n\nLS = data.loc[~cond, ]\n\n#Работа над выбросами в целевой переменной\n#LS.loc[LS['LifeSquare'] < 10, 'LifeSquare'] = 10\nLS.loc[LS['LifeSquare'] < 10, 'LifeSquare'] = 0.8*LS['Square'] - LS['KitchenSquare']\nLS.loc[LS['LifeSquare'] > LS['LifeSquare'].quantile(.975), 'LifeSquare'] = LS['LifeSquare'].median()\n#LS.loc[(LS['LifeSquare'] < 10) | (LS['LifeSquare'] > LS['LifeSquare'].quantile(.975)), 'LifeSquare'] = 0.8*LS['Square'] - LS['KitchenSquare']\n\nX_train = LS.drop(columns = target_name)\ny_train = LS[target_name]\n\n\npreprocessor = DataPreprocessing_for_LS()\npreprocessor.fit(X_train)\n\nX_train = preprocessor.transform(X_train)\nLS_nan  = preprocessor.transform(LS_nan)\n\nX_train.shape, LS_nan.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:41.01298Z","iopub.execute_input":"2021-08-04T08:45:41.013539Z","iopub.status.idle":"2021-08-04T08:45:41.128358Z","shell.execute_reply.started":"2021-08-04T08:45:41.013488Z","shell.execute_reply":"2021-08-04T08:45:41.127367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_gen = FeatureGenetator_for_LS()\nfeatures_gen.fit(X_train, y_train)\n\nX_train = features_gen.transform(X_train)\nLS_nan  = features_gen.transform(LS_nan)\n\nX_train = X_train[feature_names + new_feature_names]\nLS_nan  = LS_nan[feature_names + new_feature_names]\n\nX_train.isna().sum().sum(), LS_nan.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:43.841174Z","iopub.execute_input":"2021-08-04T08:45:43.841588Z","iopub.status.idle":"2021-08-04T08:45:43.940656Z","shell.execute_reply.started":"2021-08-04T08:45:43.841545Z","shell.execute_reply":"2021-08-04T08:45:43.939688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_model = RandomForestRegressor(max_depth = 13, n_estimators = 200, min_samples_leaf = 4 , random_state=21, criterion='mse')\n#rf_model.fit(X_train, y_train)\n\ngb_model = GradientBoostingRegressor(learning_rate = 0.1, max_depth = 5, min_samples_leaf = 4, min_samples_split = 2,\\\n                                     n_estimators = 100)\ngb_model.fit(X_train, y_train)\n\n#y_train_preds = rf_model.predict(X_train)\n#y_nan_preds = rf_model.predict(LS_nan)\n\ny_train_preds = gb_model.predict(X_train)\ny_nan_preds = gb_model.predict(LS_nan)\n\nevaluate_preds(y_train, y_train_preds, [0, 1, 2], [0, 1, 2])","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:45:52.801061Z","iopub.execute_input":"2021-08-04T08:45:52.8014Z","iopub.status.idle":"2021-08-04T08:45:55.924766Z","shell.execute_reply.started":"2021-08-04T08:45:52.80137Z","shell.execute_reply":"2021-08-04T08:45:55.923743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.loc[cond, 'LifeSquare'] = rf_model.predict(LS_nan)\ndata.loc[cond, 'LifeSquare'] = gb_model.predict(LS_nan)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:46:06.770308Z","iopub.execute_input":"2021-08-04T08:46:06.770937Z","iopub.status.idle":"2021-08-04T08:46:06.785431Z","shell.execute_reply.started":"2021-08-04T08:46:06.770898Z","shell.execute_reply":"2021-08-04T08:46:06.784687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b> Часть, выбранная для исследования. Важность остальных признаков была < 10**(-2)","metadata":{}},{"cell_type":"code","source":"feature_names = ['Square', 'LifeSquare', 'KitchenSquare', 'Floor', 'HouseFloor', 'HouseYear',\n                 'Ecology_1', 'Social_1', 'Social_2', 'Social_3']\n\nnew_feature_names = ['DistrictSize', 'MedPriceByFloorYear', 'MedPriceByDistrict']\n\ntarget_name = 'Price'","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:46:08.361155Z","iopub.execute_input":"2021-08-04T08:46:08.361749Z","iopub.status.idle":"2021-08-04T08:46:08.366436Z","shell.execute_reply.started":"2021-08-04T08:46:08.361709Z","shell.execute_reply":"2021-08-04T08:46:08.365471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = data\ntest_df = pd.read_csv(TEST_DATASET_PATH)\n\nX = train_df.drop(columns=target_name)\ny = train_df[target_name]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=21)\n\npreprocessor = DataPreprocessing()\npreprocessor.fit(X_train)\n#preprocessor.fit(X)\n\nX_train = preprocessor.transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n#X = preprocessor.transform(X)\ntest_df = preprocessor.transform(test_df)\n\n#X.shape, test_df.shape\nX_train.shape, X_valid.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:46:35.263297Z","iopub.execute_input":"2021-08-04T08:46:35.263714Z","iopub.status.idle":"2021-08-04T08:46:35.395671Z","shell.execute_reply.started":"2021-08-04T08:46:35.263671Z","shell.execute_reply":"2021-08-04T08:46:35.394783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.isna().sum().sum(), X_valid.isna().sum().sum(), test_df.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:46:40.769548Z","iopub.execute_input":"2021-08-04T08:46:40.770073Z","iopub.status.idle":"2021-08-04T08:46:40.790996Z","shell.execute_reply.started":"2021-08-04T08:46:40.770015Z","shell.execute_reply":"2021-08-04T08:46:40.789835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_gen = FeatureGenetator()\nfeatures_gen.fit(X_train, y_train)\n\n#features_gen.fit(X, y)\n\nX_train = features_gen.transform(X_train)\nX_valid = features_gen.transform(X_valid)\ntest_df = features_gen.transform(test_df)\n\n#X = features_gen.transform(X)\n#test_df = features_gen.transform(test_df)\n\nX_train = X_train[feature_names + new_feature_names]\nX_valid = X_valid[feature_names + new_feature_names]\ntest_df = test_df[feature_names + new_feature_names]\n\n#X = X[feature_names + new_feature_names]\n#test_df = test_df[feature_names + new_feature_names]\n\n#X.isna().sum().sum(), test_df.isna().sum().sum()\n\nX_train.isna().sum().sum(), X_valid.isna().sum().sum(), test_df.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:47:14.585922Z","iopub.execute_input":"2021-08-04T08:47:14.586249Z","iopub.status.idle":"2021-08-04T08:47:14.711206Z","shell.execute_reply.started":"2021-08-04T08:47:14.58622Z","shell.execute_reply":"2021-08-04T08:47:14.710257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_valid.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:24.696185Z","iopub.status.idle":"2021-08-04T08:44:24.69671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. <b>Построение модели","metadata":{}},{"cell_type":"markdown","source":"<b>Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"\n#rf_model = RandomForestRegressor(random_state=21, criterion='mse', n_jobs = 3)\n#gb_model = GradientBoostingRegressor(random_state=21, criterion='mse')\n#params = {'n_estimators' : [10, 100, 200], 'max_depth' : [el for el in range(1, 20)], 'min_samples_leaf' : [el for el in range(2, 5)]}\n#clf = GridSearchCV(gb_model, params, scoring = 'r2')\n#clf.fit(X_train, y_train)\n#clf.best_params_, clf.best_score_\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:24.697767Z","iopub.status.idle":"2021-08-04T08:44:24.698254Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_model = RandomForestRegressor(max_depth = 19, n_estimators = 200, min_samples_leaf = 3 , random_state=21, criterion='mse')\ngb_model = GradientBoostingRegressor(max_depth = 5, min_samples_leaf = 4, n_estimators = 200, random_state=21, criterion='mse')\n\n#rf_model.fit(X_train, y_train)\ngb_model.fit(X_train, y_train)\n#gb_model.fit(X, y)\n\ny_train_preds = gb_model.predict(X_train)\ny_test_preds = gb_model.predict(X_valid)\n#y_train_preds = gb_model.predict(X)\n\n#y_train_preds = rf_model.predict(X_train)\n#y_test_preds = rf_model.predict(X_valid)\n\nevaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:48:18.825767Z","iopub.execute_input":"2021-08-04T08:48:18.82615Z","iopub.status.idle":"2021-08-04T08:48:23.694901Z","shell.execute_reply.started":"2021-08-04T08:48:18.82611Z","shell.execute_reply":"2021-08-04T08:48:23.693754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cv_score = cross_val_score(rf_model, X_train, y_train, scoring='r2', cv=KFold(n_splits=3, shuffle=True, random_state=21))\ncv_score = cross_val_score(gb_model, X_train, y_train, scoring='r2', cv=KFold(n_splits=3, shuffle=True, random_state=21))\n#cv_score = cross_val_score(gb_model, X, y, scoring='r2', cv=KFold(n_splits=3, shuffle=True, random_state=21))\ncv_score","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:48:34.907267Z","iopub.execute_input":"2021-08-04T08:48:34.907676Z","iopub.status.idle":"2021-08-04T08:48:43.841184Z","shell.execute_reply.started":"2021-08-04T08:48:34.907614Z","shell.execute_reply":"2021-08-04T08:48:43.840377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_score.mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:48:43.842531Z","iopub.execute_input":"2021-08-04T08:48:43.843038Z","iopub.status.idle":"2021-08-04T08:48:43.848975Z","shell.execute_reply.started":"2021-08-04T08:48:43.843005Z","shell.execute_reply":"2021-08-04T08:48:43.847995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>StackingRegressor, VotingRegressor, BaggingRegressor","metadata":{}},{"cell_type":"code","source":"#Stacking\n\nlr = LinearRegression()\ngb = GradientBoostingRegressor(max_depth = 5, min_samples_leaf = 4, n_estimators = 200)\n\nstack = StackingRegressor([('lr', lr), ('rf', rf_model)], final_estimator=gb)\nstack.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:24.703347Z","iopub.status.idle":"2021-08-04T08:44:24.703791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_preds = stack.predict(X_train)\ny_test_preds = stack.predict(X_valid)\n\nevaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:44:24.704709Z","iopub.status.idle":"2021-08-04T08:44:24.705134Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.<b> Прогнозирование на тестовом датасете","metadata":{}},{"cell_type":"code","source":"submit = pd.read_csv('/kaggle/input/real-estate-price-prediction-moscow/sample_submission.csv')\npredictions = gb_model.predict(test_df)\nsubmit['Price'] = predictions\nsubmit.to_csv('gb_v1_submit.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T08:49:07.595039Z","iopub.execute_input":"2021-08-04T08:49:07.59541Z","iopub.status.idle":"2021-08-04T08:49:07.665394Z","shell.execute_reply.started":"2021-08-04T08:49:07.595373Z","shell.execute_reply":"2021-08-04T08:49:07.664343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}